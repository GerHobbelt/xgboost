#' Show importance of features in a model
#'
#' Create a \code{list} of the most important features of a model.
#'
#' @param feature_names names of each feature as a \code{character} vector. Can be extracted from a sparse matrix (see example). If model dump already contains feature names, this argument should be \code{NULL}.
#' @param filename_dump character. Filename of the xgboost dump generated by the \code{xgb.dump} function.
#' @return A \code{list} of the features used in the model with their relative importance in the model.
#'
#' @details
#'
#' This implementation of variable importance algorithm is agnostic of the type of tree that is built.
#' For each variable, we calculate the number of splits that are made on this variable and assign is as the
#' importance value for the aforementioned variable. This turns out to be a pretty good proxy for
#' how influential a variable's change are on the resulting model score.
#' @export
xgb.importance <- function(feature_names, filename_dump) {
  fmap <- list()
  nmap <- list()
  sapply(seq_along(feature_names),
    function(x) {
      fmap[[feature_names[x]]] <<- 0
      nmap[[x]] <<- feature_names[x]
     })
  total <- 0
  sapply(readLines(filename_dump),
    function(x) {
      m <- regexec("\\[f.*\\]", x)
      p <- regmatches(x, m)
      if (length(p[[1]]) > 0) {
        splits <- strsplit(sub("\\]", "", sub("\\[f", "", p[[1]])), "<")[[1]]
        fmap[[nmap[[as.integer(splits[1]) + 1]]]] <<- fmap[[nmap[[as.integer(splits[1]) + 1]]]] + 1
        total <<- total + 1
        }
    })
  stopifnot(total > 0)
  lapply(fmap, function(x) x / total * 100)
}
